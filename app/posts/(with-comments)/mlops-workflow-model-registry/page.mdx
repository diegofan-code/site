---
title: "Part6 - Key Concepts to MLOps Professional Must Know - Model Registry"
date: 2025/07/18
description: This Article will tell about Model Registry concept and helps to answer Interviews basic questions.
thumbnail: /images/posts/mlops-workflow-model-registry/modelregistry.png
tags: [MLOps]
author: Diego Fan Ribeiro 
---

import Image from 'next/image'

# MLOps Workflow Structure: Model Registry


![MLOps Workflow Structure: Model Registry](/images/posts/mlops-workflow-model-registry/modelregistry.png)

#### **Concept** :

A system to **manage model lifecycle** (development â†’ staging â†’ production).

#### **Key Concepts** :

- **Staging Environments** : Testing models in staging before production.
- **Model Metadata** : Tracking accuracy, training date, and framework (e.g., TensorFlow 2.10).
- **Approval Workflows** : Requiring sign-offs before deployment.

#### **Example** :

A healthcare startup registers a diabetes prediction model as `v1` (staging). After validation, itâ€™s promoted to `v1-production`. If performance drops, they roll back to `v0`.

#### **Challenges** :

- Handling multiple model versions in production (A/B testing).
- Ensuring backward compatibility (e.g., new model versions donâ€™t break APIs).

- **What it is** : A system to manage trained models (staging, production, archiving).
- **Key Tools** :
    - **MLflow Model Registry** .
    - **TensorFlow Serving** /**TorchServe** .
    - **AWS SageMaker Model Registry** .
- **Best Practices** :
    - Tag models with metadata (e.g., accuracy, training date).
    - Implement **approval workflows** before deployment.
- **Interview Prep** :
    - How do you roll back to a previous model version in production?
    - Compare MLflow vs. TensorFlow Serving for model serving.


Rolling back to a previous model version in production is a critical process to **mitigate failures** (e.g., performance degradation, bugs, or unintended biases) while ensuring minimal downtime. Hereâ€™s a structured approach, with **examples** and **best practices** :

---

### **Key Steps to Roll Back a Model**

#### **1. Identify the Target Version**

- **Why** : Determine which version to revert to (e.g., the last stable version).
- **How** :
    
    - Check **model registry** metadata (e.g., accuracy, deployment date, version tags).
    - Review **experiment tracking** logs to compare performance metrics.
    
- **Example** :
    - Production uses `model_v3` (accuracy=88%), but performance drops. Roll back to `model_v2` (accuracy=92%).

---

#### **2. Use a Model Registry**

- **Why** : A registry tracks all model versions and their metadata.
- **How** :
    
    - Retrieve the desired version (e.g., `model_v2`) from the registry.
    - Validate its compatibility with current data/features.
    
- **Example** :
    - MLflowâ€™s Model Registry tags `model_v2` as "production-ready."

---

#### **3. Update Deployment Configuration**

- **Why** : Redirect traffic to the rolled-back version.
- **How** :
    
    - **Blue-Green Deployment** : Keep `model_v2` running alongside `model_v3` and switch routing.
    - **Canary Rollback** : Gradually shift traffic back to `model_v2`.
    
- **Example** :
    - Use Kubernetes to update the service endpoint from `model-v3` to `model-v2`.

---

#### **4. Test the Rollback**

- **Why** : Ensure the rolled-back model works with **current data** and **dependencies** .
- **How** :
    
    - Run **smoke tests** (e.g., validate input/output formats).
    - Check for **data/schema compatibility** (e.g., `model_v2` expects 10 features, but new data has 12).
    
- **Example** :
    - Test `model_v2` with live traffic in a staging environment before full rollout.

---

#### **5. Monitor Post-Rollback**

- **Why** : Confirm the rollback resolved the issue.
- **How** :
    
    - Track metrics like accuracy, latency, and business KPIs (e.g., user engagement).
    - Set up alerts for anomalies.
    
- **Example** :
    - After rolling back, monitor a 24-hour window to ensure `model_v2` stabilizes fraud detection rates.

---

#### **6. Document and Investigate**

- **Why** : Prevent future incidents.
- **How** :
    - Log the rollback in **metadata** (e.g., "Rolled back to v2 due to data drift").
    - Investigate why `model_v3` failed (e.g., training-serving skew).

---

### **Example Scenario**

**Problem** : A recommendation model (`v3`) starts suggesting irrelevant products, causing a 20% drop in click-through rate.  
**Rollback Process** :

1. Identify `v2` (last stable version) in the model registry.
2. Use a blue-green deployment to switch traffic to `v2`.
3. Validate `v2` works with current user features (e.g., no schema changes).
4. Monitor metrics to confirm recovery.
5. Investigate `v3`â€™s failure (e.g., overfitting to stale data).

---

### **Challenges & Solutions**

- **Data Drift** :
    
    - _Problem_ : `model_v2` was trained on older data that no longer matches current patterns.
    - _Solution_ : Retrain `v2` on recent data or use a **feature store** to ensure consistency.
    
- **Dependencies** :
    - _Problem_ : `model_v2` relies on deprecated libraries.
    - _Solution_ : Containerize models (e.g., Docker) to isolate dependencies.

---

### **Best Practices**

1. **Automate Rollbacks** : Integrate rollback triggers into CI/CD pipelines (e.g., auto-rollback if latency > 500ms).
2. **Keep Models "Warm"** : Pre-deploy older versions in shadow mode to reduce downtime.
3. **Version Everything** : Track models, data, and code together (e.g., link `model_v2` to `dataset_2023-01`).

---

### **Interview Answer**

_"To roll back a model, Iâ€™d first identify the stable version using a model registry. For example, if `model_v3` degrades, Iâ€™d retrieve `model_v2` from the registry and use a blue-green deployment to switch traffic. Iâ€™d validate compatibility with current data and monitor post-rollout metrics. Finally, Iâ€™d document the incident and analyze why `v3` failedâ€”like checking for data drift or code bugs."_

This approach balances speed, safety, and learning, ensuring minimal disruption to users. ðŸš€



Here's a **detailed comparison** of **MLflow** and **TensorFlow Serving** for model serving, focusing on their strengths, use cases, and trade-offs:

---

### **1. Primary Purpose**

|**Aspect**|**MLflow**|**TensorFlow Serving**|
|---|---|---|
|**Focus**|End-to-end ML lifecycle management (tracking, packaging, deployment).|High-performance serving of**TensorFlow/Keras models**in production.|
|**Scope**|General-purpose tool for**any ML framework**(Scikit-learn, PyTorch, etc.).|Specialized for**TensorFlow models**with optimizations for speed and scalability.|

---

### **2. Framework Support**

|**Aspect**|**MLflow**|**TensorFlow Serving**|
|---|---|---|
|**Compatibility**|âœ… Works with**any framework**(via`mlflow.<framework>.log_model()`).|âœ… Only supports**TensorFlow/Keras models**.|
|**Flexibility**|Ideal for teams using**multiple frameworks**(e.g., XGBoost + PyTorch).|Limited to TensorFlow; not suitable for heterogeneous environments.|

**Example** :

- Use MLflow to serve a Scikit-learn model for fraud detection.
- Use TensorFlow Serving for a TensorFlow-based image classification model.

---

### **3. Deployment & APIs**

|**Aspect**|**MLflow**|**TensorFlow Serving**|
|---|---|---|
|**APIs**|REST API for model inference.|**gRPC**(high-performance) and REST APIs.|
|**Ease of Use**|Simple deployment via`mlflow models serve`(great for prototyping).|Requires manual setup (Docker/Kubernetes) but optimized for production.|
|**Latency**|Higher overhead due to generic REST endpoints.|Ultra-low latency (batching, parallelism).|

**Example** :

- MLflow: Quick REST API deployment for a small-scale NLP model.
- TensorFlow Serving: Serves thousands of image requests/sec with gRPC.

---

### **4. Scalability & Performance**

|**Aspect**|**MLflow**|**TensorFlow Serving**|
|---|---|---|
|**Scaling**|Relies on external tools (e.g., Kubernetes) for horizontal scaling.|Built-in**multi-model serving**and**dynamic batching**for high throughput.|
|**Resource Use**|Moderate efficiency (generic serving layer).|Optimized for**GPUs/TPUs**and minimal memory overhead.|

**Use Case** :

- TensorFlow Serving is better for large-scale applications (e.g., recommendation systems at Netflix).
- MLflow is better for small-to-medium workloads or multi-framework deployments.

---

### **5. Versioning & Updates**

|**Aspect**|**MLflow**|**TensorFlow Serving**|
|---|---|---|
|**Versioning**|Tracks versions in its**Model Registry**(metadata, stages, tags).|Automatically manages versions (e.g.,`model/1`,`model/2`) with**A/B testing**.|
|**Rolling Updates**|Requires manual orchestration (e.g., Kubernetes deployments).|Seamless updates with**zero downtime**(keeps old versions warm during transitions).|

**Example** :

- MLflow: Manually update a modelâ€™s stage from "Staging" to "Production" via the UI.
- TensorFlow Serving: Deploy `model_v2` while keeping `model_v1` live for canary testing.

---

### **6. Integration & Ecosystem**

|**Aspect**|**MLflow**|**TensorFlow Serving**|
|---|---|---|
|**Ecosystem**|Integrates with**AWS SageMaker**,**Azure ML**, and**Kubeflow**.|Tightly coupled with**TensorFlow Extended (TFX)**for end-to-end pipelines.|
|**Customization**|Extensible via plugins (e.g., custom deployment targets).|Limited to TensorFlowâ€™s ecosystem but offers**advanced optimizations**(e.g., XLA).|

---

### **7. Use Cases**

|**Scenario**|**Best Tool**|**Why**|
|---|---|---|
|**Multi-framework environment**|MLflow|Supports Scikit-learn, PyTorch, etc., under one system.|
|**TensorFlow-only production**|TensorFlow Serving|Optimized for TensorFlowâ€™s performance and scalability.|
|**Rapid prototyping**|MLflow|Easy REST API deployment with minimal setup.|
|**Large-scale, low-latency systems**|TensorFlow Serving|gRPC, batching, and GPU/TPU optimizations.|
|**Model lifecycle management**|MLflow|Tracks experiments, versions, and deployments in one place.|

---

### **Key Takeaways**

- **MLflow** :
    
    - Best for **flexibility** (multi-framework) and **end-to-end ML workflows** .
    - Use if you need a unified platform for tracking, packaging, and deploying models.
    
- **TensorFlow Serving** :
    
    - Best for **TensorFlow models at scale** .
    - Use if you need **ultra-low latency** , **high throughput** , or advanced TensorFlow integrations.

---

### **Interview Answer**

MLflow and TensorFlow Serving serve different needs. For a team using multiple frameworks (e.g., PyTorch and XGBoost), MLflowâ€™s flexibility is ideal. It also simplifies deployment through its Model Registry and REST API. However, for TensorFlow-specific use cases requiring maximum performance like real-time image recognition TensorFlow Servingâ€™s gRPC support, batching, and GPU optimizations make it the better choice. The decision ultimately depends on your stack and scale requirements.

This comparison highlights how both tools complement each other in different scenarios. ðŸš€

### **Reflections**

The Model Registry is an essential component of any MLOps workflow, enabling the management, versioning, and deployment of machine learning models. Both TensorFlow and MLflow serve distinct purposes within the ML ecosystem. If you're working on a project involving computer vision or complex deep learning architectures, TensorFlow is often the preferred choice due to it's robust support for building, training, and deploying neural networks. On the other hand, MLflow is particularly well-suited for managing the full ML lifecycle, especially in scenarios involving simpler, traditional machine learning models. Choosing the right tool depends on the specific requirements of the project, including model complexity, deployment needs, and overall pipeline design.

### References

- [Data Intensive Book](https://dataintensive.net/)
- [Hidden Technical Debt in Machine Learning Systems](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)
- [Continuous Training of Machine Learning Models](https://blog.stackademic.com/continuous-training-of-ml-models-7d8acaf44dda)
- [Tfx: A tensorflow-based production-scale machine learning pipeline framework](https://dl.acm.org/doi/10.1145/3097983.3098021)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Share Features](https://docs.tecton.ai/docs/share-features#feature-sharing-patterns)
- [Uber Engineering Blog](https://www.uber.com/en-BR/blog/salvador/engineering/)
- [Netflix Tech Blog](https://netflixtechblog.com/)
- [Towards Data Science (Medium)](https://towardsdatascience.com/)
- [arxiv](https://arxiv.org/)
- [Machine Learning Operations: A Survey on MLOps Tool Support](https://arxiv.org/abs/2202.10169)
- [Building Machine Learning Powered Applications](https://www.oreilly.com/library/view/building-machine-learning/9781492045106/)
- [Feast Documentation](https://docs.feast.dev/)